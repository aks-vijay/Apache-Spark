{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKirxW3Gy/6sVJkx8mfhT2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aks-vijay/Apache-Spark/blob/main/Ingest%2C_Wrangle_%26_Export_F1_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
        "from pyspark.sql.functions import current_timestamp, col, lit, concat\n",
        "import re\n",
        "\n",
        "class ProccessData:\n",
        "    def __init__(self, datapath, schema) -> None:\n",
        "        self.spark = SparkSession.builder \\\n",
        "            .appName(\"MyLocalSparkApp\") \\\n",
        "            .master(\"local[*]\") \\\n",
        "            .getOrCreate()\n",
        "        self.df = self.spark.read \\\n",
        "            .schema(schema=schema) \\\n",
        "            .option(\"header\", True) \\\n",
        "            .csv(datapath) \\\n",
        "            .withColumn(\"ingestion_date\", current_timestamp())\n",
        "\n",
        "      # create mapping for country codes\n",
        "\n",
        "        self.country_codes = {\n",
        "            \"Russia\": \"RU\",\n",
        "            \"Sweden\": \"SE\",\n",
        "            \"Malaysia\": \"MY\",\n",
        "            \"Singapore\": \"SG\",\n",
        "            \"Turkey\": \"TR\",\n",
        "            \"Germany\": \"DE\",\n",
        "            \"France\": \"FR\",\n",
        "            \"Argentina\": \"AR\",\n",
        "            \"Belgium\": \"BE\",\n",
        "            \"China\": \"CN\",\n",
        "            \"India\": \"IN\",\n",
        "            \"Italy\": \"IT\",\n",
        "            \"Spain\": \"ES\",\n",
        "            \"Monaco\": \"MC\",\n",
        "            \"Morocco\": \"MA\",\n",
        "            \"USA\": \"US\",\n",
        "            \"Mexico\": \"MX\",\n",
        "            \"Azerbaijan\": \"AZ\",\n",
        "            \"UK\": \"UK\",\n",
        "            \"Saudi Arabia\": \"SA\"\n",
        "        }\n",
        "        self.country_codes_lists = list(self.country_codes.items())\n",
        "        self.country_codes_lookup = self.spark.createDataFrame(self.country_codes_lists, schema=[\"country\", \"country_codes\"])\n",
        "\n",
        "    def extract_all_columns (self, df) -> List:\n",
        "      return [column for column, datatype in df.dtypes]\n",
        "\n",
        "    def extract_string_columns(self, df) -> List:\n",
        "        return [column for column, datatype in df.dtypes if datatype == \"string\"]\n",
        "\n",
        "    def extract_integer_columns(self, df) -> List:\n",
        "        return [column for column, datatype in df.dtypes if datatype == \"int\"]\n",
        "\n",
        "    def camel_to_snake(self, column):\n",
        "        return re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', column).lower()\n",
        "\n",
        "    def extract_columns_and_convert_to_snake(self):\n",
        "        for col in self.df.columns:\n",
        "            new_col_name = self.camel_to_snake(col)\n",
        "            self.df = self.df.withColumnRenamed(col, new_col_name)\n",
        "        return self.df\n",
        "\n",
        "    def write_to_file(self, df, datapath):\n",
        "        export_file_name = datapath.replace(\".csv\", \".parquet\")\n",
        "        df.write \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .parquet(export_file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ynb4EGYeZ9A",
        "outputId": "350b8194-adc3-45eb-a5da-e614c64d0bb6"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define user defined schema\n",
        "circuits_user_defined_schema = StructType(fields=\n",
        "    [\n",
        "        StructField('circuitId', IntegerType(), False),\n",
        "        StructField('circuitRef', StringType(), True),\n",
        "        StructField('name', StringType(), True),\n",
        "        StructField('location', StringType(), True),\n",
        "        StructField('country', StringType(), True),\n",
        "        StructField('lat', DoubleType(), True),\n",
        "        StructField('lng', DoubleType(), True),\n",
        "        StructField('alt', IntegerType(), True),\n",
        "        StructField('url', StringType(), True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "races_user_defined_schema = StructType(fields=\n",
        "                                        [StructField('raceId', IntegerType(), False),\n",
        "                                          StructField('year', IntegerType(), True),\n",
        "                                          StructField('round', IntegerType(), True),\n",
        "                                          StructField('circuitId', IntegerType(), False),\n",
        "                                          StructField('name', StringType(), True),\n",
        "                                          StructField('date', DateType(), True),\n",
        "                                          StructField('time', StringType(), True),\n",
        "                                          StructField('url', StringType(), True)]\n",
        "                                       )\n",
        "\n",
        "\n",
        "circuits_input_file_path = \"/content/circuits.csv\"\n",
        "races_input_file_path = \"/content/races.csv\"\n",
        "\n",
        "# initialize  process to Spark dataframe\n",
        "circuits_data_processor = ProccessData(\n",
        "                                        datapath=circuits_input_file_path,\n",
        "                                        schema = circuits_user_defined_schema\n",
        "                                      )\n",
        "\n",
        "races_data_processor = ProccessData(\n",
        "                                      datapath = races_input_file_path,\n",
        "                                      schema = races_user_defined_schema\n",
        "                                  )\n",
        "\n",
        "# data cleaning - rename the columns -> convert to snake case\n",
        "df_renamed_circuits = circuits_data_processor.extract_columns_and_convert_to_snake()\n",
        "df_renamed_races = races_data_processor.extract_columns_and_convert_to_snake()\n",
        "\n",
        "# clean the dataframes\n",
        "df_races_cleaned = df_renamed_races \\\n",
        "                    .withColumn(\"race_date_time\", concat(col(\"date\"), lit(\" \"), col(\"time\"))) \\\n",
        "                    .drop(\"date\", col(\"time\"))\n",
        "\n",
        "columns_to_select = [\"circuit_id\", \"circuit_ref\", \"name\", \"location\", \"country\", \"location_and_country\", \"lat\", \"lng\", \"alt\", \"url\"]\n",
        "\n",
        "df_circuits_cleaned = df_renamed_circuits \\\n",
        "                        .withColumn(\"location_and_country\", concat(col(\"location\"), lit(\", \"), col(\"country\"))) \\\n",
        "                        .join(circuits_data_processor.country_codes_lookup, df_renamed_circuits.country==circuits_data_processor.country_codes_lookup.country, \"inner\") \\\n",
        "                        .drop(df_renamed_circuits[\"country\"], circuits_data_processor.country_codes_lookup[\"country\"]) \\\n",
        "                        .withColumnRenamed(\"country_codes\", \"country\") \\\n",
        "                        .select(*columns_to_select)\n",
        "\n",
        "df_races_circuits_cleaned = df_races_cleaned \\\n",
        "                              .join(df_circuits_cleaned, df_races_cleaned.circuit_id == df_circuits_cleaned.circuit_id, \"inner\") \\\n",
        "                              .select(df_races_cleaned.race_id,\n",
        "                                      df_races_cleaned.year.alias(\"race_year\"),\n",
        "                                      df_races_cleaned.round,\n",
        "                                      df_races_cleaned.circuit_id,\n",
        "                                      df_races_cleaned.name.alias(\"race_name\"),\n",
        "                                      df_races_cleaned.race_date_time,\n",
        "                                      df_circuits_cleaned.name.alias(\"circuit_name\"),\n",
        "                                      df_circuits_cleaned.location.alias(\"circuit_location\"),\n",
        "                                      df_circuits_cleaned.country.alias(\"circuit_country\"),\n",
        "                                      df_circuits_cleaned.lat.alias(\"latitude\"),\n",
        "                                      df_circuits_cleaned.lng.alias(\"longitude\"),\n",
        "                                      df_races_cleaned.url.alias(\"race_url\"),\n",
        "                                      df_circuits_cleaned.url.alias(\"circuit_url\"),\n",
        "                                      df_races_cleaned.ingestion_date)\n",
        "\n",
        "# export to parquet\n",
        "circuits_data_processor.write_to_file(df_races_circuits_cleaned, circuits_input_file_path)"
      ],
      "metadata": {
        "id": "L_wFEwdsfKog"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SpbW97NBxxuH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}