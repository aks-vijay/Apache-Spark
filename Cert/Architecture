Execution Modes:
- Local - Runs the driver and cluster manager in the same local
- Client - Runs the driver on client machine.
- Cluster - Runs the driver on worker nodes.

Who schedules task on a cluster in Client mode?
- Spark driver schedules tasks on the cluster. 

Cluster manager is typically on a seperate node. Not on the same host as the driver.

Spark UI
Executor page shows driver and worker details. How much each core it's using.
Environment - Consolidated view of relevant properties

Executors (Workers) and Slots:

Executors:
- If you configure the workers as 3 in the cluster configuration, there will be 3 executors
- Each executor will have 4 cores to process data (Depending on the configuration)

Slot:
- Core is called as SLOT. When data is not being processed, it's called as SLOT. 
- If 4 is the core and 3 executors, then total capicity with respect to total slots would be 12 (3 executors x 4 cores)
- Driver decides which task should be done and allocates it to empty slots. If a slot is busy, then driver will allocate to other empty slots.
- Data is converted to Task as soon as the data is being processed by the slot
- If a driver submits task that requires 80 tasks, at a time only 12 tasks will be processed. 1 task per core. Once task is completed, slots will be available to take additional tasks.

Task:
- Deepest level of Spark Execution

Adaptive Execution:
- If a shuffle operation happens, the job will use entire paritions to process a small data file if adaptive execution is disabled.
- If enabled, it will use lesser number of paritions for a shuffle
- It also optimizes partiion pruning

# adaptive execution
spark.conf.get("spark.sql.adaptive.enabled")

# get parition
spark.conf.get("spark.sql.shuffle.partitions")

High level categories of adaptive execution:
- Coalescing post shuffle partitions (property: spark.sql.adaptive.coalesce.paritions.enabled)
- Converting sort merge join to broadcast join (property: spark.sql.adaptive.autoBroadcastJoinThreshold)
- Converting sort merge join to shuffled hash join (property: spark.sql.adaptive.maxShuffledHashjoinLocalMapThreshold)
- Optimizing Skew join (property: spark.sql.adaptive.skewJoin.enabled)

Dataset API
- Dataset API is not available in Python. Only available in Scala.

Caching
- Saves the dataframe in the memory. If we have to refer to the dataframe again, it does a lookup to the cached dataframe instead of reading the whole dataframe
- df.cache()

To learn more:
Persist
Broadcast join
RDD
load
asc, desc - sort/orderby
size
dropduplicates
Random rows  
drop columns - multiple
Option - nullvalue, emptyvalue
Shared variables
Memory management and storage levels

dataframe has no broadcast method
Wrong â€“ Jobs, Stages, Storage, Executors, and SQL are all tabs in the Spark UI. DAGs can be inspected in the "Jobs" tab in the job details or in the Stages or SQL tab, but are not a separate tab.

All of these features, except for dynamically injecting scan filters, are part of Adaptive Query Execution. Dynamically injecting scan filters for join operations to limit the amount of data to be considered in a query is part of Dynamic Partition Pruning and not of Adaptive Query Execution
Explode - Can be called only from select. Not a method to be accessed by DF
Collect - Does not take any arguments. 
Head - Can be used if the column type is correct
Pow - Works in Spark
Array_contains - Works in filter even with strings
Boolean types - only | and & are valid
df.drop ("", "") - Even if the column is unavailable, Spark ignores, If list is used, we need to use varying arguments
df.printSchema - Wrong. Should be called with brackets as its a method
UDF registration - (Name of the UDF, {Function}, ReturnType)

The code block displayed below contains an error. When the code block below has executed, it should have divided DataFrame transactionsDf into 14 parts, based on columns storeId and transactionDate (in this order). Find the error.

Code block:

transactionsDf.coalesce(14, ("storeId", "transactionDate"))

Explanation
Correct code block:

transactionsDf.repartition(14, "storeId", "transactionDate").count()

Since we do not know how many partitions DataFrame transactionsDf has, we cannot safely use coalesce, since it would not make any change if the current number of partitions is smaller than 14. So, we need to use repartition.

In the Spark documentation, the call structure for repartition is shown like this: DataFrame.repartition(numPartitions, *cols). The * operator means that any argument after numPartitions will be interpreted as column. Therefore, the brackets need to be removed.

Finally, the question specifies that after the execution the DataFrame should be divided. So, indirectly this question is asking us to append an action to the code block. Since .select() is a transformation. the only possible choice here is .count().
